{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Если данное решение Вам помогло, не забудьте поставить ^ (upvote) в правом верхнем углу","metadata":{}},{"cell_type":"markdown","source":"Импорт библиотек","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport zipfile\nimport cv2\nfrom matplotlib import pyplot as plt\nimport shutil \nfrom tqdm import tqdm\nimport torch\nimport torchvision\nimport time\nimport copy\nfrom torchvision import transforms, models","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:32:05.191299Z","iopub.execute_input":"2021-07-05T15:32:05.191655Z","iopub.status.idle":"2021-07-05T15:32:05.197257Z","shell.execute_reply.started":"2021-07-05T15:32:05.191619Z","shell.execute_reply":"2021-07-05T15:32:05.196315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Распаковка исходных данных","metadata":{}},{"cell_type":"code","source":"print(os.listdir('../input'),'\\n') \nwith zipfile.ZipFile('../input/platesv2/plates.zip', 'r') as zip_obj:\n        zip_obj.extractall('/kaggle/working/') \nprint(os.listdir('/kaggle/working/'))\n\ndata_root = '/kaggle/working/plates/' \nprint(data_root)\nprint(os.listdir(data_root))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-05T15:32:07.983443Z","iopub.execute_input":"2021-07-05T15:32:07.983764Z","iopub.status.idle":"2021-07-05T15:32:08.384831Z","shell.execute_reply.started":"2021-07-05T15:32:07.983726Z","shell.execute_reply":"2021-07-05T15:32:08.383824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Удаляем фон  - remove_background\n2. Находим координаты центра фигуры - findCoordinates\n3. Ищем круги в пределах координат - find_circle\n4. Обрезаем изображения для увеличения train датасета - crop \n5. Для test датасета оставляем изображение вырезанное по вписанному (в найденный круг) квадрату - crop_test","metadata":{}},{"cell_type":"code","source":"class Remove_background_and_crop:\n    \n    def __init__(self,img):\n        self.x00 = 0\n        self.x00 = 0\n        self.r00 = 0\n        self.img = img\n        self.mask = img\n                \n    def crop (self): \n            c_r_crop = (1.42*self.r00/2)\n            self.img = self.img[int(self.y00)-int(c_r_crop):int(self.y00)+int(c_r_crop),int(self.x00)-int(c_r_crop):int(self.x00)+int(c_r_crop)]\n            self.img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)\n            crop = self.img\n            cv2.imwrite(image_folder,crop)\n            h,w = self.img.shape[:2] \n            c = min(h,w)     \n            for i in range (5,int(c/3),5): \n                    crop_img = self.img[i:h-i,i:w-i]    \n                    cv2.imwrite(image_folder[:-4] +  '_Crop_' + str(i) + '.jpg',crop_img)\n            image1 = self.img[0:int(h//2),0:int(w//2)]\n            #Делим изображение на 4 части\n            cv2.imwrite(image_folder[:-4] +  'image1' + '.jpg',image1)\n            image2 = self.img[int(h//2):h,int(w//2):w]\n            cv2.imwrite(image_folder[:-4] +  'image2' + '.jpg',image2)\n            image3 = self.img[int(h//2):h,0:int(w//2)]\n            cv2.imwrite(image_folder[:-4] +  'image3' + '.jpg',image3)\n            image4 = self.img[0:int(h//2),int(w//2):w]\n            cv2.imwrite(image_folder[:-4] +  'image4' + '.jpg',image4)\n            \n    def crop_test (self):\n        c_r_crop = (1.42*self.r00/2)\n        self.img = self.img[int(self.y00)-int(c_r_crop):int(self.y00)+int(c_r_crop),int(self.x00)-int(c_r_crop):int(self.x00)+int(c_r_crop)]\n        self.img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)\n        cv2.imwrite(image_folder,self.img)       \n            \n    def find_circle(self):\n        output = self.img.copy()    \n        img = cv2.convertScaleAbs(self.img, alpha=1.2, beta=0.0)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 10,param1=10,param2=5,minRadius=40,maxRadius=250)\n        \n        if circles is not None: \n            #print('Координаты центра:',self.x00,self.y00)\n            circles = np.round(circles[0, :]).astype(\"int\")\n            #print('Возможные координаты центра и радиусы:')\n            #print(circles) \n                             \n            for x, y, r in circles:\n                if ((self.x00-15)<x<(self.x00+15)) and ((self.y00-15)<y<(self.y00+15)):\n                    if r > self.r00: \n                        self.x00 = x\n                        self.y00 = y\n                        self.r00 = r\n                        #print('Найдено совпадение:',x,y,r)\n                    \n            if self.r00==0: \n                #print('Тарелка не найдена','\\n')\n                ret,thresh = cv2.threshold(self.mask,235,255,0)\n                contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)   \n                c = max(contours, key=cv2.contourArea)\n                (self.x00, self.y00), self.r00 = cv2.minEnclosingCircle(c)\n            #print('Выбранные координаты центра:',self.x00,self.y00,self.r00)\n            \n    def findCoordinates(self):  \n        ret,thresh = cv2.threshold(self.mask,235,255,0)        \n        M = cv2.moments(thresh)\n        self.x00 = int(M[\"m10\"] / M[\"m00\"])\n        self.y00 = int(M[\"m01\"] / M[\"m00\"])\n        \n#    def findCenter(self):\n#        c_r_crop = 124\n#        self.img = self.img[int(self.y00)-int(c_r_crop):int(self.y00)+int(c_r_crop),int(self.x00)-int(c_r_crop):int(self.x00)+int(c_r_crop)]\n                \n    def remove_background(self):  \n        mainRectSize = .08\n        fgSize = .01\n        img = self.img\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        new_h, new_w = img.shape[:2]\n        mask = np.zeros(img.shape[:2], np.uint8)\n        bg_w = round(new_w * mainRectSize)\n        bg_h = round(new_h * mainRectSize)\n        bg_rect = (bg_w, bg_h, new_w - bg_w, new_h - bg_h)\n        fg_w = round(new_w * (1 - fgSize) / 2)\n        fg_h = round(new_h * (1 - fgSize) / 2)\n        fg_rect = (fg_w, fg_h, new_w - fg_w, new_h - fg_h)\n        cv2.rectangle(mask, fg_rect[:2], fg_rect[2:4], color=cv2.GC_FGD, thickness=-1)\n        bgdModel1 = np.zeros((1, 65), np.float64)\n        fgdModel1 = np.zeros((1, 65), np.float64)\n        cv2.grabCut(img, mask, bg_rect, bgdModel1, fgdModel1, 3, cv2.GC_INIT_WITH_RECT)\n        cv2.rectangle(mask, bg_rect[:2], bg_rect[2:4], color=cv2.GC_PR_BGD, thickness=bg_w * 3)\n        cv2.grabCut(img, mask, bg_rect, bgdModel1, fgdModel1, 10, cv2.GC_INIT_WITH_MASK)   \n        mask_result = np.where((mask == 1) + (mask == 3), 255, 0).astype('uint8')\n        masked = cv2.bitwise_and(img, img, mask=mask_result)\n        masked[mask_result < 2] = [255, 255, 255] \n        self.img = masked\n        self.mask = mask_result\n    \nfor image_index in range (20):\n    print (\"Complete dirty: \",\"{0:04}\".format(image_index),\"/0019\", end=\"\\r\")\n    image_folder = '/kaggle/working/plates/train/dirty/{0:04}.jpg'.format(image_index) \n    img = cv2.imread(image_folder)\n    out_img  = Remove_background_and_crop(img)\n    out_img.remove_background()\n    out_img.findCoordinates()\n    out_img.find_circle()\n    out_img.crop()    \nprint (\"\\n\\r\", end=\"\")    \n    \nfor image_index in range (20):\n    print (\"Complete cleaned: \",\"{0:04}\".format(image_index),\"/0019\", end=\"\\r\")\n    image_folder = '/kaggle/working/plates/train/cleaned/{0:04}.jpg'.format(image_index) \n    img = cv2.imread(image_folder)\n    out_img  = Remove_background_and_crop(img)\n    out_img.remove_background()\n    out_img.findCoordinates()\n    out_img.find_circle()\n    out_img.crop()\nprint (\"\\n\\r\", end=\"\")\n\nfor image_index in range (744):\n    print (\"Complete test: \",\"{0:04}\".format(image_index),\"/0743\", end=\"\\r\")\n    image_folder = '/kaggle/working/plates/test/{0:04}.jpg'.format(image_index) \n    img = cv2.imread(image_folder)\n    out_img  = Remove_background_and_crop(img)\n    out_img.remove_background()\n    out_img.findCoordinates()\n    out_img.find_circle()\n    out_img.crop_test()\nprint (\"\\n\\r\", end=\"\") ","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:32:10.36462Z","iopub.execute_input":"2021-07-05T15:32:10.364956Z","iopub.status.idle":"2021-07-05T16:03:08.422277Z","shell.execute_reply.started":"2021-07-05T15:32:10.364924Z","shell.execute_reply":"2021-07-05T16:03:08.420409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Создаем папку train и val (из папки train берем каждую шестую на валидацию)","metadata":{}},{"cell_type":"code","source":"train_dir = 'train' # на этих данных будем обучать модель\nval_dir = 'val' #на этих данных будем смотреть какую accuracy показывает наша модель \nclass_names = ['cleaned', 'dirty']\n \nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n \nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 6 != 0:\n            dest_dir = os.path.join(train_dir, class_name) \n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:03:08.424491Z","iopub.execute_input":"2021-07-05T16:03:08.424857Z","iopub.status.idle":"2021-07-05T16:03:08.493711Z","shell.execute_reply.started":"2021-07-05T16:03:08.424817Z","shell.execute_reply":"2021-07-05T16:03:08.492942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for class_name in class_names:\n     source_dir = os.path.join(\"/kaggle/working/\", 'val', class_name)\n     for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        dest_dir = os.path.join(train_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:26:44.825253Z","iopub.execute_input":"2021-07-05T16:26:44.825587Z","iopub.status.idle":"2021-07-05T16:26:44.854851Z","shell.execute_reply.started":"2021-07-05T16:26:44.825558Z","shell.execute_reply":"2021-07-05T16:26:44.854173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Аугментация данных","metadata":{}},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.RandomPerspective(distortion_scale=0.09, p=0.75, interpolation=3, fill=255),\n    transforms.Resize((224, 224)),    \n    transforms.ColorJitter(hue=(-0.5,0.5)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(), \n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #Транформация нормировки. Изображения привести к виду, на котором обучался изначальный ResNet. Вычетаем от красного, зелёного и синего константы 0.485, 0.456, 0.406 и делим на 0.229, 0.224, 0.225 (данные из мануала ResNet)\n])\n\nval_transforms = transforms.Compose([\n    transforms.RandomPerspective(distortion_scale=0.1, p=0.8, interpolation=3, fill=255),\n    transforms.Resize((224, 224)),\n    transforms.ColorJitter(hue=(-0.5,0.5)),\n    transforms.RandomHorizontalFlip(),     \n    transforms.RandomVerticalFlip(), \n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #Транформация нормировки. Изображения привести к виду, на котором обучался изначальный ResNet. Вычетаем от красного, зелёного и синего константы 0.485, 0.456, 0.406 и делим на 0.229, 0.224, 0.225 (данные из мануала ResNet)\n])   \n\ndataset_transforms = {\n                      'orig': transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n\n                      '140': transforms.Compose([\n    transforms.CenterCrop(140),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                     '135': transforms.Compose([\n    transforms.CenterCrop(135),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]), \n                      '130': transforms.Compose([\n    transforms.CenterCrop(130),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '125': transforms.Compose([\n    transforms.CenterCrop(125),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '120': transforms.Compose([\n    transforms.CenterCrop(120),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '115': transforms.Compose([\n    transforms.CenterCrop(115),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '110': transforms.Compose([\n    transforms.CenterCrop(110),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '105': transforms.Compose([\n    transforms.CenterCrop(105),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '100': transforms.Compose([\n    transforms.CenterCrop(100),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                     '95': transforms.Compose([\n    transforms.CenterCrop(95),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                       '90': transforms.Compose([\n    transforms.CenterCrop(90),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                       '85': transforms.Compose([\n    transforms.CenterCrop(85),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                       '80': transforms.Compose([\n    transforms.CenterCrop(80),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '75': transforms.Compose([\n    transforms.CenterCrop(75),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),                                         \n                       '70': transforms.Compose([\n    transforms.CenterCrop(70),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),                                                           \n                     }\n \ntrain_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\nval_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n\nbatch_size = 16 # Количество изображений в батче\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:29:16.398943Z","iopub.execute_input":"2021-07-05T16:29:16.399287Z","iopub.status.idle":"2021-07-05T16:29:16.448889Z","shell.execute_reply.started":"2021-07-05T16:29:16.399254Z","shell.execute_reply":"2021-07-05T16:29:16.44788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader), len(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:29:16.620178Z","iopub.execute_input":"2021-07-05T16:29:16.620558Z","iopub.status.idle":"2021-07-05T16:29:16.626962Z","shell.execute_reply.started":"2021-07-05T16:29:16.620527Z","shell.execute_reply":"2021-07-05T16:29:16.62581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Функция вывода изображений","metadata":{}},{"cell_type":"code","source":"def show_input(input_tensor, title=''):\n    image = input_tensor.permute(1, 2, 0).numpy() #обратная операция к ToTensor - .permute(1, 2, 0) Channels,H,W -> H,W,Channels потом превращаем в numpy array .numpy()\n    image = std * image + mean #Делаем обратную трансформацию нормировки\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()\n    plt.pause(0.1)\n \nX_batch, y_batch = next(iter(train_dataloader)) \nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, title=class_names[y_item])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:29:21.803618Z","iopub.execute_input":"2021-07-05T16:29:21.803948Z","iopub.status.idle":"2021-07-05T16:29:30.052547Z","shell.execute_reply.started":"2021-07-05T16:29:21.803917Z","shell.execute_reply":"2021-07-05T16:29:30.051832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train","metadata":{}},{"cell_type":"code","source":"def train_model(model, loss, optimizer, scheduler, num_epochs):\n \n    loss_hist = {'train':[], 'val':[]}\n    acc_hist = {'train':[], 'val':[]}\n \n    for epoch in range(num_epochs):\n        print(\"Epoch {}/{}:\".format(epoch, num_epochs - 1), end=\"\")\n        for phase in ['train', 'val']:\n            if phase == 'train': #Если фаза == Тренировки  \n                dataloader = train_dataloader #берем train_dataLoader\n                scheduler.step() #Делаем 1 шаг (произошла одна эпоха)\n                model.train()  # Модель в training mode - обучение (Фиксируем модель, иначе у нас могут изменяться параметры слоя батч-нормализации и изменится нейронка с течением времени)\n            else: #Если фаза == Валидации \n                dataloader = val_dataloader #берем val_dataLoader \n                model.eval()   # Модель в evaluate mode - валидация (Фиксируем модель, иначе у нас могут изменяться параметры слоя батч-нормализации и изменится нейронка с течением времени)\n \n            running_loss = 0. \n            running_acc = 0.\n \n            # Итерируемся по dataloader\n            for inputs, labels in tqdm(dataloader):\n                inputs = inputs.to(device) # Тензор с изображениями переводим на GPU \n                labels = labels.to(device) # Тензор с лейблами переводим на GPU \n \n                optimizer.zero_grad() # Обнуляем градиент,чтобы он не накапливался \n \n                with torch.set_grad_enabled(phase == 'train'): #Если фаза train то активируем все градиенты (те которые не заморожены) (очистить историю loss)\n                    preds = model(inputs) # Считаем предикты, input передаем в модель\n                    loss_value = loss(preds, labels) #Посчитали  Loss    \n                    preds_class = preds.argmax(dim=1) # Получаем класс,берем .argmax(dim=1) нейрон с максимальной активацией\n                \n                    if phase == 'train':\n                        loss_value.backward() # Считаем градиент \n                        optimizer.step() # Считаем шаг градиентного спуска\n \n                # Статистика\n                running_loss += loss_value.item() #считаем Loss\n                running_acc += (preds_class == labels.data).float().mean().data.cpu().numpy()  #считаем accuracy\n \n            epoch_loss = running_loss / len(dataloader)  # Loss'ы делим на кол-во бачей в эпохе \n            epoch_acc = running_acc / len(dataloader) #считаем Loss на кол-во бачей в эпохе\n \n            print(\"{} Loss: {:.4f} Acc: {:.4f} \".format(phase, epoch_loss, epoch_acc), end=\"\")\n            \n            loss_hist[phase].append(epoch_loss)\n            acc_hist[phase].append(epoch_acc)\n        \n    return model, loss_hist, acc_hist","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:29:30.056458Z","iopub.execute_input":"2021-07-05T16:29:30.056712Z","iopub.status.idle":"2021-07-05T16:29:30.07431Z","shell.execute_reply.started":"2021-07-05T16:29:30.056685Z","shell.execute_reply":"2021-07-05T16:29:30.073329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Параметры модели","metadata":{}},{"cell_type":"code","source":"class FocalLoss(torch.nn.modules.loss._WeightedLoss):\n    def __init__(self, weight=None, gamma=2,reduction='mean'):\n        super(FocalLoss, self).__init__(weight,reduction=reduction)\n        self.gamma = gamma\n        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n\n    def forward(self, input, target):\n        ce_loss = torch.nn.functional.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n        pt = torch.exp(-ce_loss)\n        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n        return focal_loss","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:29:30.075792Z","iopub.execute_input":"2021-07-05T16:29:30.076241Z","iopub.status.idle":"2021-07-05T16:29:30.088523Z","shell.execute_reply.started":"2021-07-05T16:29:30.076182Z","shell.execute_reply":"2021-07-05T16:29:30.087782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.resnet152(pretrained=True) #Формат pretrained=True - нам нужны веса, которые получились вследствие обучения этого ResNet, на датасете ImageNet\n          \n# Замораживаем веса, чтобы не использовать лишние веса в обучении, а обучать только последний слой\nfor param in model.parameters(): #Проходим по параметрам модели (каждый параметр - это каждый слой, model.parameters нам отдаст некоторый итератор по слоям)\n   param.requires_grad = False #Для каждого параметра и слоя:\"requires grad = False\", то есть уже не требуется вычисление градиента для данного слоя. И получается, что у нас вся сетка будет заморожена, то есть мы не сможем вообще ничего обучать.\n    \n#Меняем последний полносвязанный слой, в ResNet он классифицирует на тысячу классов, а у нас класса всего 2.\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2) # Cоздадим слой torch.nn.Linear, это полносвязный слой, на вход он принимает model fc_in features. И он единсвенный - разморожен.\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n \n#Определяем Loss функцию\n#В данном случае - это бинарная кросс-энтропия CrossEntropyLoss (у нас всего 2 класса) \nloss = FocalLoss () #torch.nn.CrossEntropyLoss()\n# Метод градиентного спуска Adam\noptimizer = torch.optim.Adam(model.parameters(), amsgrad=True, lr=0.001) #lr - (learning rate - шаг градиентного спуска)\n# Уменьшаем шаг градиентного спуска каждые 7 эпох\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:29:30.09031Z","iopub.execute_input":"2021-07-05T16:29:30.090695Z","iopub.status.idle":"2021-07-05T16:29:31.867916Z","shell.execute_reply.started":"2021-07-05T16:29:30.090667Z","shell.execute_reply":"2021-07-05T16:29:31.8669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Запускаем обучение","metadata":{}},{"cell_type":"code","source":"model, loss, acc = train_model(model, loss, optimizer, scheduler, num_epochs=50); #Запуск функции Train (Модель= ResNet,Loss-функция= CrossEntropyLoss(бинарная кросс-энтропия),Метод градиентного спуска= Adam, Уменьшение градиентного спуска в зависимости от кол-ва эпох, Кол-во эпох= 30) )","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:29:31.870533Z","iopub.execute_input":"2021-07-05T16:29:31.870906Z","iopub.status.idle":"2021-07-05T16:35:54.045021Z","shell.execute_reply.started":"2021-07-05T16:29:31.870862Z","shell.execute_reply":"2021-07-05T16:35:54.043895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"График Accuracy","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (14, 7)\nfor experiment_id in acc.keys():\n    plt.plot(acc[experiment_id], label=experiment_id)\nplt.legend(loc='upper left')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch num', fontsize=15)\nplt.ylabel('Accuracy value', fontsize=15);\nplt.grid(linestyle='--', linewidth=0.5, color='.7')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:35:54.050611Z","iopub.execute_input":"2021-07-05T16:35:54.051441Z","iopub.status.idle":"2021-07-05T16:35:54.346653Z","shell.execute_reply.started":"2021-07-05T16:35:54.051392Z","shell.execute_reply":"2021-07-05T16:35:54.345712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"График loss","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (14, 7)\nfor experiment_id in loss.keys():\n    plt.plot(loss[experiment_id], label=experiment_id)\nplt.legend(loc='upper left')\nplt.title('Model Loss')\nplt.xlabel('Epoch num', fontsize=15)\nplt.ylabel('Loss function value', fontsize=15)\nplt.grid(linestyle='--', linewidth=0.5, color='.7')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:35:54.350585Z","iopub.execute_input":"2021-07-05T16:35:54.352759Z","iopub.status.idle":"2021-07-05T16:35:54.536759Z","shell.execute_reply.started":"2021-07-05T16:35:54.352716Z","shell.execute_reply":"2021-07-05T16:35:54.535931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Костыль ImageFolder, который не может обработать путь к папке в которой уже сразу лежат изображения\n#Копируем всю папку test в директорию test\\unknown\n#test_dir = 'test'\n#shutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:35:54.538013Z","iopub.execute_input":"2021-07-05T16:35:54.538518Z","iopub.status.idle":"2021-07-05T16:35:54.582945Z","shell.execute_reply.started":"2021-07-05T16:35:54.538479Z","shell.execute_reply":"2021-07-05T16:35:54.581327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Мы не знаем, какие ID, какие названия изображения у нас генерируется, когда мы просим у DataLoader -- \"дай нам следующий батч\".\n#Они по алфавиту идут, по дате создания, или просто случайным образом -- непонятно.\n#Поэтому нам нужно переписать немножко ImageFolder, чтобы он нам отдавал не просто tuple, с самим изображением и его меткой, а ещё, чтобы он отдавал имя, ну, либо -- путь к изображению.\nclass ImageFolderWithPaths(torchvision.datasets.ImageFolder): #Создаем класс, он наследуется от ImageFolder, но изменяет его функцию get_item\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index) #Дополняем original_tuple путем для файла (.__getitem__(index))\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path\n    \ndf = pd.DataFrame\n#Итерируемся по Crop'ам test датасета\nfor (i,tranforms) in dataset_transforms.items():\n    test_dataset = ImageFolderWithPaths('/kaggle/working/test', tranforms) #Берем новый класс и получаем tuple из 3х значений\n    test_dataloader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=0) #Новый даталоадер с путями до изображений\n    \n    model.eval() #Переводим модель в состояние eval\n    test_predictions = []  #Создаем пустой список предсказания \n    test_img_paths = [] #Пути до изображения\n    for inputs, labels, paths in tqdm(test_dataloader): #Цикл по test_dataloader inputs - батч с изображением, lable - тут none, paths - пути до изображения  \n        inputs = inputs.to(device) \n        labels = labels.to(device)  \n        with torch.set_grad_enabled(False):\n            preds = model(inputs) # Считаем предикшены\n        test_predictions.append(\n            torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy()) #С помощью torch.nn.functional.softmax получаем вероятности, для первого класса [:,1], пеереводим тензор в .data, на .cpu(), в numpy \n        test_img_paths.extend(paths)\n    test_predictions = np.concatenate(test_predictions)\n    \n    \n    submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})\n    submission_df['id'] = submission_df['id'].str.replace('/kaggle/working/test/unknown/', '')\n    submission_df['id'] = submission_df['id'].str.replace('.jpg', '')\n    submission_df.set_index('id', inplace=True)\n    \n    try : df = df.merge(submission_df, how='inner', on='id') #Объединяем в один датафрейм\n    except BaseException: # Для первой итерации\n        df = submission_df \n    #submission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.50 else 'cleaned')\n    #submission_df.to_csv('submission_predict_{0}.csv'.format(i))\ndf.head(8)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:39:31.313356Z","iopub.execute_input":"2021-07-05T16:39:31.313682Z","iopub.status.idle":"2021-07-05T16:40:41.954062Z","shell.execute_reply.started":"2021-07-05T16:39:31.313651Z","shell.execute_reply":"2021-07-05T16:40:41.952983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Получаем среднее по всем crop'ам","metadata":{}},{"cell_type":"code","source":"df['mean'] = df.mean(axis=1)\ndf.drop(df.columns[:-1], axis='columns', inplace=True)\ndf['label'] = df['mean'].map(lambda pred: 'dirty' if pred > 0.50 else 'cleaned')\ndf.drop(df.columns[:-1], axis='columns', inplace=True)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:40:41.955919Z","iopub.execute_input":"2021-07-05T16:40:41.956299Z","iopub.status.idle":"2021-07-05T16:40:41.974483Z","shell.execute_reply.started":"2021-07-05T16:40:41.95626Z","shell.execute_reply":"2021-07-05T16:40:41.97346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сохраняем","metadata":{}},{"cell_type":"code","source":"df.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T16:40:41.975802Z","iopub.execute_input":"2021-07-05T16:40:41.97617Z","iopub.status.idle":"2021-07-05T16:40:41.982432Z","shell.execute_reply.started":"2021-07-05T16:40:41.976133Z","shell.execute_reply":"2021-07-05T16:40:41.98113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf train val","metadata":{"execution":{"iopub.status.busy":"2021-07-05T14:16:44.348819Z","iopub.execute_input":"2021-07-05T14:16:44.34916Z","iopub.status.idle":"2021-07-05T14:16:45.027545Z","shell.execute_reply.started":"2021-07-05T14:16:44.349122Z","shell.execute_reply":"2021-07-05T14:16:45.026511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}